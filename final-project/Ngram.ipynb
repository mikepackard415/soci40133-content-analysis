{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c1b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc80803",
   "metadata": {},
   "source": [
    "### Read in environmental discourse data, drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c363784",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pd.read_csv('../Data/Environmental Discourse/env.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab175d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env[['source', 'url', 'title', 'date', 'author', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "575183ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env[~env.duplicated(subset='url')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6033dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env[~env.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff48789e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93763, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49cdd86",
   "metadata": {},
   "source": [
    "### Clean up the text a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30405bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \n",
    "    #text = text.replace('Ed. note: ', '')\n",
    "    #text = text.replace(' Grist thanks its sponsors. Become one.', '')\n",
    "    #text = text.replace('\\xa0', ' ')\n",
    "    #text = text.replace('\\n', '')\n",
    "    text = text.replace('Click on the headline (link) for the full text', '')\n",
    "    text = text.replace('Many more articles are available through the Energy Bulletin homepage', '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "env['text'] = env.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a02355e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.to_csv('../Data/Environmental Discourse/env.csv') # 2/28/22 10:21 pm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de64d8a",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "Now I'm going to work with a small sample so that I can move quickly and make decisions, and then run things on the RCC. Hopefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126381ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: jinja2 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: setuptools in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/mcpackard/anaconda3/envs/thesis/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e04441b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en\")\n",
    "except OSError:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94601103",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pd.read_csv('../Data/Environmental Discourse/env.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03d07243",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env.sample(1000, random_state=827)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "614f2bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93763, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "612cee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    \n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\", \"lemmatizer\"])\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5453226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list.lower(), disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization and normalization functions\n",
    "env['tokenized_text'] = env['text'].apply(lambda x: word_tokenize(x))\n",
    "env['normalized_tokens'] = env['tokenized_text'].apply(lambda x: normalizeTokens(x, lemma=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env['bigrams'] = env['normalized_tokens'].apply(lambda x: [i for i in ngrams(x, 2)])\n",
    "bigrams = pd.Series(env['bigrams'].sum()).value_counts().head(100)\n",
    "bigram_df = pd.DataFrame({'bigram': bigrams})\n",
    "bigram_df.to_csv('../Data/Environmental Discourse/bigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5651033",
   "metadata": {},
   "outputs": [],
   "source": [
    "env['trigrams'] = env['normalized_tokens'].apply(lambda x: [i for i in ngrams(x, 3)])\n",
    "trigrams = pd.Series(env['trigrams'].sum()).value_counts().head(100)\n",
    "trigram_df = pd.DataFrame({'trigram': trigrams})\n",
    "trigram_df.to_csv('../Data/Environmental Discourse/trigrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.to_pkl('../Data/Environmental Discourse/env_toknorm.pkl') # 2/28/22 9:59 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38ac85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcf5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tok = pd.read_pickle('../Data/Environmental Discourse/env_tok.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348617d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tok.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c55d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
